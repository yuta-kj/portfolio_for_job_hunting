{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#version_3\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-12T15:28:05.474319Z","iopub.execute_input":"2022-03-12T15:28:05.47477Z","iopub.status.idle":"2022-03-12T15:28:05.503676Z","shell.execute_reply.started":"2022-03-12T15:28:05.474688Z","shell.execute_reply":"2022-03-12T15:28:05.502974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor as rfr\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\")\ny = train.target\nX = train.drop(['id','target'], axis=1)\nencoder = OneHotEncoder(categories = 'auto')\ny_enc = encoder.fit_transform(y.values.reshape(X.shape[0],1)).toarray()\nX_train, X_valid, y_train, y_valid = train_test_split(X, y_enc, train_size=0.8, test_size=0.2, random_state=0)\n#regressor\n\n# params = {'n_estimators': [int(x) for x in np.linspace(start = 20, stop = 300, num = 50)],\n#           'max_features': [int(x) for x in np.linspace(10, 110, num = 11)],\n#           'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n#           'min_samples_split': [2, 5, 10],\n#           'min_samples_leaf': [1, 2, 4],\n#           'bootstrap': [True, False]\n#           }\n# rf = RandomForestRegressor()\n\n# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n# 損失関数はdefault=”squared_error”MSE\n# 評価関数はr2\n\n#classify\nrfc = rfc()\nparams = {\"n_estimators\":[int(x) for x in np.linspace(start = 20, stop = 300, num = 50)],\n#           \"criterion\": [\"gini”, “entropy”],\n          \"max_depth\": [int(x) for x in np.linspace(10, 110, num = 11)],\n          \"min_samples_split\": [2, 5, 10],\n          \"min_samples_leaf\": [1, 2, 4],\n#           \"min_weight_fraction_leaf\": ,\n#           \"max_features\":,\n#           \"max_leaf_nodes\":,\n#           \"min_impurity_decrease\":,\n#           \"bootstrap\":,\n          \"n_jobs\": [2],\n          \"random_state\": [42],\n          \"verbose\": [10],\n#           \"warm_start\":,\n#           \"class_weight\":,\n#           \"ccp_alpha\":,\n#           \"max_samples\":\n          }\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-30T08:42:38.330828Z","iopub.execute_input":"2022-03-30T08:42:38.33136Z","iopub.status.idle":"2022-03-30T08:42:41.753111Z","shell.execute_reply.started":"2022-03-30T08:42:38.331266Z","shell.execute_reply":"2022-03-30T08:42:41.752345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2022-05-17T06:37:30.496229Z","iopub.execute_input":"2022-05-17T06:37:30.496500Z","iopub.status.idle":"2022-05-17T06:37:30.505774Z","shell.execute_reply.started":"2022-05-17T06:37:30.496467Z","shell.execute_reply":"2022-05-17T06:37:30.504964Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = params, n_iter = 3, cv = 3, verbose=100, random_state=42)#, n_jobs = 2)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T04:11:36.6992Z","iopub.execute_input":"2022-03-16T04:11:36.699539Z","iopub.status.idle":"2022-03-16T04:11:36.706221Z","shell.execute_reply.started":"2022-03-16T04:11:36.699505Z","shell.execute_reply":"2022-03-16T04:11:36.704649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_random.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T04:11:42.381536Z","iopub.execute_input":"2022-03-16T04:11:42.381878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n\n# RandomForsetClassifier_method\n#----\n# apply(X)\n# decision_path(X)\n# fit(X, y[, sample_weight])\n# get_params([deep])\n# predict(X)\n# predict_log_proba(X)\n# predict_proba(X)\n# score(X, y[, sample_weight])\n# set_params(**params)\n\n#RandomForsetClassifier_Attributes\n#----\n# base_estimator_\n# estimators_\n# classes_\n# n_classes_\n# n_features_\n# n_features_in_\n# feature_names_in_\n# n_outputs_\n# feature_importances_\n# oob_score_\n# oob_decision_function_","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV.score\n\n#RandomizedSearchCV_methods\n#----\n# decision_function(X)\n#  Call decision_function on the estimator with the best found parameters.\n# fit(X[, y, groups])\n#  Run fit with all sets of parameters.\n# get_params([deep])\n#  Get parameters for this estimator.\n# inverse_transform(Xt)\n#  Call inverse_transform on the estimator with the best found params.\n# predict(X)\n#  Call predict on the estimator with the best found parameters.\n# predict_log_proba(X)\n#  Call predict_log_proba on the estimator with the best found parameters.\n# predict_proba(X)\n#  Call predict_proba on the estimator with the best found parameters.\n# score(X[, y])\n#  Return the score on the given data, if the estimator has been refit.\n# score_samples(X)\n#  Call score_samples on the estimator with the best found parameters.\n# set_params(**params)\n#  Set the parameters of this estimator.\n# transform(X)\n\n#RandomizedSearchCV_attributes\n# ----\n# cv_results_\n# best_estimator_\n# best_score_\n# best_params_\n# best_index_\n# scorer_\n# n_splits_\n# refit_time_\n# feature_names_in_","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBのパラメータ\n # General parameters：共通の動作に関するパラメーター\n # Booster parameters：弱学習器の種類に依存するパラメーター\n # Learning task parameters：学習についてのパラメーター\n # Command line parameters：コマンドラインからのパラメーター\n\n# General parameters\n# booster [default= gbtree ]\n # 弱学習器として何を使うか\n# silent [default=0] [Deprecated]\n # ログに関する制御\n # 現在は非推奨なので、verbosity を使うこと\n# verbosity [default=1]\n # ログに関する制御\n # 0 (silent), 1 (warning), 2 (info), 3 (debug)\n# nthread [default to maximum number of threads available if not set]\n # スレッド数\n # デフォルトでは CPU スレッド数\n# disable_default_eval_metric [default=0]\n # Set to >0 to disable.\n# num_pbuffer [set automatically by XGBoost, no need to be set by user]\n # 予想値を保存するバッファのサイズ\n # XGBoost が自動で設定するので指定不要\n# num_feature [set automatically by XGBoost, no need to be set by user]\n # 特徴量の数\n # XGBoost が自動で設定するので指定不要\n\n# Booster Parameters(for Tree Booster)\n# eta [default=0.3, alias: learning_rate]\n # 作成したモデルの効果に係数 η をかけて弱める\n # y^(t)i=y^(t−1)i+ηft(xi)\n# gamma [default=0, alias: min_split_loss]\n # 正則化項に出てきた γ\n# max_depth [default=6]\n # 木の深さの最大値\n# min_child_weight [default=1]\n # 葉に割り当てるスコア wi の合計の最小値\n # これを下回った場合、それ以上の分割を行わない\n# max_delta_step [default=0]\n# subsample [default=1]\n # 木を構築する前にデータのサブサンプリングを行う比率\n # 1 なら全部のデータを使うし、0.5 なら半分のデータを使う\n# colsample_bytree, colsample_bylevel, colsample_bynode [default=1]\n # 列のサブサンプリングを行う比率\n # 2 つ以上を指定した場合は累計的に機能する\n # たとえば、64の特徴量があった場合、{'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} では、各分割で 8 つの特徴量が使われる\n # range of (0, 1]\n # サブサンプリングを行うタイミングごとに別れている\n # colsample_bytree : 各木を構築する前に行われる (木ごとに一回のみ)\n # colsample_bylevel : 木の深さごとに1回づつ行われる\n # colsample_bynode : ノードごとに1回づつ行われる\n# lambda [default=1, alias: reg_lambda]\n # L2 正則化項の係数として出てきた λ\n# alpha [default=0, alias: reg_alpha]\n # L1 正則化項の係数\n# tree_method string [default= auto]\n# sketch_eps [default=0.03]\n    # tree_method=approx の場合にのみ使われる変数\n    # range: (0, 1)\n# scale_pos_weight [default=1]\n # 正と負の w のバランスを制御します。正負のデータ数に違いがある場合に役立つ\n # 典型的には、(負例の数) / (正例の数) などを指定\n# updater [default= grow_colmaker,prune]\n # 木のアップデーター\n # これは高度なパラメータで、通常は他のパラメータに応じて自動的に設定される\n# refresh_leaf [default=1]\n # updater に refresh を指定した場合に使われる値\n# process_type [default= default]\n # ブースティングプロセスの種類\n# grow_policy [default= depthwise]\n # 新しいノードをツリーに追加する方法を制御\n # tree_method=hist の場合にのみサポート\n# max_leaves [default=0]\n # 葉の最大数\n# max_bin, [default=256]\n # ビンの最大数\n # tree_method=hist の場合にのみ有効\n# predictor, [default=cpu_predictor]\n # 予想を行う際のアルゴリズム (CPU で実行するか、GPU で実行するか)\n# num_parallel_tree, [default=1]\n # 各ブーストステップで構築される木の数\n # 各ステップでランダムフォレストを使う場合は 1 より大きな値\n\n# Learning Task Parameters\n# objective [default=reg:squarederror]\n # 目的関数\n# base_score [default=0.5]\n # 予想の初期値\n# eval_metric [default according to objective]\n # 評価指標\n # 特に指定しなかった場合、objective と同じ指標が使われます\n# seed [default=0]\n # 乱数シード","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Learning Task Parameters\n# objective [default=reg:squarederror]\n\n# reg:squarederror: regression with squared loss.\n\n# reg:squaredlogerror: regression with squared log loss \n \n# . All input labels are required to be greater than -1. Also, see metric rmsle for possible issue with this objective.\n\n# reg:logistic: logistic regression\n\n# reg:pseudohubererror: regression with Pseudo Huber loss, a twice differentiable alternative to absolute loss.\n\n# binary:logistic: logistic regression for binary classification, output probability\n\n# binary:logitraw: logistic regression for binary classification, output score before logistic transformation\n\n# binary:hinge: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\n\n# count:poisson –poisson regression for count data, output mean of Poisson distribution\n\n# max_delta_step is set to 0.7 by default in Poisson regression (used to safeguard optimization)\n\n# survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR).\n\n# survival:aft: Accelerated failure time model for censored survival time data. See Survival Analysis with Accelerated Failure Time for details.\n\n# aft_loss_distribution: Probability Density Function used by survival:aft objective and aft-nloglik metric.\n\n# multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n\n# multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class.\n\n# rank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n\n# rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n\n# rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized\n\n# reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed.\n\n# reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed.","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Learning Task Parameters\n# eval_metric [default according to objective]\n\n# Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and logloss for classification, mean average precision for ranking)\n\n# User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter eval_metric won’t override previous one\n\n# The choices are listed below:\n\n# rmse: root mean square error\n\n# rmsle: root mean square log error: \n \n# . Default metric of reg:squaredlogerror objective. This metric reduces errors generated by outliers in dataset. But because log function is employed, rmsle might output nan when prediction value is less than -1. See reg:squaredlogerror for other requirements.\n\n# mae: mean absolute error\n\n# mape: mean absolute percentage error\n\n# mphe: mean Pseudo Huber error. Default metric of reg:pseudohubererror objective.\n\n# logloss: negative log-likelihood\n\n# error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\n\n# error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’.\n\n# merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases).\n\n# mlogloss: Multiclass logloss.\n\n# auc: Receiver Operating Characteristic Area under the Curve. Available for classification and learning-to-rank tasks.\n\n# When used with binary classification, the objective should be binary:logistic or similar functions that work on probability.\n\n# When used with multi-class classification, objective should be multi:softprob instead of multi:softmax, as the latter doesn’t output probability. Also the AUC is calculated by 1-vs-rest with reference class weighted by class prevalence.\n\n# When used with LTR task, the AUC is computed by comparing pairs of documents to count correctly sorted pairs. This corresponds to pairwise learning to rank. The implementation has some issues with average AUC around groups and distributed workers not being well-defined.\n\n# On a single machine the AUC calculation is exact. In a distributed environment the AUC is a weighted average over the AUC of training rows on each node - therefore, distributed AUC is an approximation sensitive to the distribution of data across workers. Use another metric in distributed environments if precision and reproducibility are important.\n\n# If input dataset contains only negative or positive samples the output is NaN.\n\n# aucpr: Area under the PR curve. Available for binary classification and learning-to-rank tasks.\n\n# ndcg: Normalized Discounted Cumulative Gain\n\n# map: Mean Average Precision\n\n# ndcg@n, map@n: ‘n’ can be assigned as an integer to cut off the top positions in the lists for evaluation.\n\n# ndcg-, map-, ndcg@n-, map@n-: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions.\n\n# poisson-nloglik: negative log-likelihood for Poisson regression\n\n# gamma-nloglik: negative log-likelihood for gamma regression\n\n# cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression\n\n# gamma-deviance: residual deviance for gamma regression\n\n# tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter)\n\n# aft-nloglik: Negative log likelihood of Accelerated Failure Time model. See Survival Analysis with Accelerated Failure Time for details.\n\n# interval-regression-accuracy: Fraction of data points whose predicted labels fall in the interval-censored labels. Only applicable for interval-censored data. See Survival Analysis with Accelerated Failure Time for details.","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoostやLightGBMの n_estimators ( num_boosting_rounds )\n# やKerasの epochs をパラメータサーチの対象にしてはいけません。\n# https://amalog.hateblo.jp/entry/hyper-parameter-search","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LearningAPIではxgboost.train()\n# sklearnAPIではxgboost.fit() ","metadata":{"execution":{"iopub.status.busy":"2022-03-19T09:49:28.004167Z","iopub.execute_input":"2022-03-19T09:49:28.004451Z","iopub.status.idle":"2022-03-19T09:49:28.008987Z","shell.execute_reply.started":"2022-03-19T09:49:28.0044Z","shell.execute_reply":"2022-03-19T09:49:28.007608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBを試す\n# PythonAPI（sklearn like）\ncvなし","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\")\ny_train = train.target\nX_train = train.drop(['id','target'], axis=1)\nlbe = LabelEncoder() #ラベルエンコード  二次元での注意　https://gotutiyan.hatenablog.com/entry/2020/09/08/122621\ny_train_lbe = lbe.fit_transform(y_train) #.values.reshape(X.shape[0],1)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train_lbe, train_size=0.8, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T07:50:15.099261Z","iopub.execute_input":"2022-03-27T07:50:15.099913Z","iopub.status.idle":"2022-03-27T07:50:16.557139Z","shell.execute_reply.started":"2022-03-27T07:50:15.099872Z","shell.execute_reply":"2022-03-27T07:50:16.556432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgboost.XGBClassifierのパラメータ\n\n# class xgboost.XGBClassifier(*, objective='binary:logistic', use_label_encoder=True, **kwargs)\n# 目的関数の選択はターゲット変数の分布に大きく依存\n# https://stackoverflow.com/questions/51337109/whats-the-different-between-objective-functions-and-xgboost-models\n# if you use a booster of the GBLinear type, you should use binary:logistic objective function. GBtree gives a decision tree modeling to your problem.\n# XGBoostは、正確に言うと勾配ブースティングであり、勾配ブースティング木ではない\n# https://qiita.com/FJyusk56/items/0649f4362587261bd57a\n#https://nykergoto.hatenablog.jp/entry/2019/03/29/%E5%8B%BE%E9%85%8D%E3%83%96%E3%83%BC%E3%82%B9%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0%E3%81%A7%E5%A4%A7%E4%BA%8B%E3%81%AA%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E6%B0%97%E6%8C%81%E3%81%A1\n\nparams = {\n    \"n_estimators\": 50, #★わざと大きい値にして過学習と未学習を抑制、fit()のearly_stopping_roundsで調整\n    \"use_label_encoder\": False, #Trueは非推奨\n    \"max_depth\": 6, #★決定木の最大の深さ\n    \"learning_rate\": 0.1, #★\n    \"verbosity\": 1,\n    \"objective\": , #★　デフォルトで'binary:logistic'\n    \"booster\": \"gbtree\",\n    \"tree_method\": \"gpu_hist\", #[exact, approx, hist, gpu_hist]\n    \"n_jobs\": -1,\n    \"gamma\": ,#決定木は葉が多くなるほど複雑になり過学習を起こしやすい。葉の数に対するペナルティ。\n    \"min_child_weight\":1,#決定木の葉の重みの下限。下限以下ならそれ以上の分割は行われない。値が大きいほど過学習の抑制になる。\n    \"max_delta_step\":0,#値を設定することで各決定木の重みの推定に制約をかけれる。あまり使用されないみたい。\n    \"subsample\":1,#各決定木においてランダムに抽出される標本の割合。1ならすべて使用。\n    \"colsample_bytree\":1,#各決定木においてランダムに抽出される列の割合。1ならすべての説明変数を使用する。\n    \"colsample_bynode\":1,#決定木の各レベル単位での分割における列のsubsample比率。決定木が分割するたびに説明変数を抽出する。subsampleとcolsample_bytreeを設置していればこれをチューニングする必要はない。\n    \"reg_alpha\":,#L1正則化に相当\n    \"reg_lambda\":,#L2正則化に相当\n    \"scale_pos_weight\":,# 不均衡データの際に、0以上の値を取ることで、収束を早めることができる。\n    \"base_score\":0.5,\n    \"random_state\":42,\n    \"missing\":,\n    \"num_parallel_tree\":,\n    \"monotone_constraints\":,\n    \"interaction_constraints\":,\n    \"importance_type\":,\n    \"gpu_id\":,\n    \"validate_parameters\":,\n    \"predictor\":'gpu_predictor',\n    \"enable_categorical\":,\n    \"max_cat_to_onehot \":,\n    \"eval_metric\":, \n    \"early_stopping_rounds\":,# early_stopping_roundsの評価指標, .fit()のeval_setも定義する必要がある。\n    \"callbacks\":,\n    \"kwargs\":\n    \"use_label_encoder\":,#★defaltでFalse\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# メモ\n# control overfiting\n# モデルの複雑さをコントロールするために　max_depth, min_child_weight,gamma\n# ノイズに対する堅牢さを高めるために　sub_sample, colsample_bytree\n\n# deal with imbalanced data\n# データのバランスをコントロールするために以下3つを試してみなさいと。\n# scale_pos_weight使ってデータのバランスを調整\n# AUCを評価指標に使う\n# max_delta_stepを\"1\"辺りに設定する\n\n# trust the cross validation\n# CVを行ってnroundを決める場合には、十分な回数を指定しつつearly.stop.roundを使って終わらせる\n# Overfittingが見られたら、etaを少なくして、nroundを大きくすることを同時にする\n# 個別のオススメの機能としては、xgb.cv()で\"prediction = TRUE\"にすることで、予測値のベクトルを返してくれるようになります。\n\n\n# どうやら以下のパラメータはAPIに関わらず共有されおり、boosterに関わらず共通、共通ではないパターンがあるので注意\n# General parameters relate to which booster we are using to do boosting, commonly tree or linear model\n# Booster parameters depend on which booster you have chosen\n# Learning task parameters decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\n# Command line parameters relate to behavior of CLI version of XGBoost.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\n# xgboost.XGBClassifierのパラメータ\n\n# class xgboost.XGBClassifier(*, objective='binary:logistic', use_label_encoder=True, **kwargs)\n# 目的関数の選択はターゲット変数の分布に大きく依存\n# https://stackoverflow.com/questions/51337109/whats-the-different-between-objective-functions-and-xgboost-models\n# if you use a booster of the GBLinear type, you should use binary:logistic objective function. GBtree gives a decision tree modeling to your problem.\n# XGBoostは、正確に言うと勾配ブースティングであり、勾配ブースティング木ではない\n# https://qiita.com/FJyusk56/items/0649f4362587261bd57a\n#https://nykergoto.hatenablog.jp/entry/2019/03/29/%E5%8B%BE%E9%85%8D%E3%83%96%E3%83%BC%E3%82%B9%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0%E3%81%A7%E5%A4%A7%E4%BA%8B%E3%81%AA%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E6%B0%97%E6%8C%81%E3%81%A1\n\nparams = {\n    \"n_estimators\": 50, #★わざと大きい値にして過学習と未学習を抑制、fit()のearly_stopping_roundsで調整\n    \"max_depth\": 6, #★決定木の最大の深さ\n    \"learning_rate\": 0.1, #★\n    \"verbosity\": 1,\n    \"objective\": 'multi:softprob', #https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n    \"booster\": \"gbtree\", #gbtree, gblinear or dart.\n    \"tree_method\": \"gpu_hist\", #[exact, approx, hist, gpu_hist]\n    \"n_jobs\": -1,\n    \"gamma\": 0.2 ,#決定木は葉が多くなるほど複雑になり過学習を起こしやすい。葉の数に対するペナルティ。\n    \"min_child_weight\":1,#決定木の葉の重みの下限。下限以下ならそれ以上の分割は行われない。値が大きいほど過学習の抑制になる\n    \"subsample\":0.8,#各決定木においてランダムに抽出される標本の割合。1ならすべて使用。\n    \"colsample_bytree\":0.8,#各決定木においてランダムに抽出される列の割合。1ならすべての説明変数を使用する。    \n    \"reg_lambda\":0.8,#L2正則化に相当 \n    \"base_score\":0.5,\n    \"random_state\":42,\n    \"missing\":\"NAN\", #欠損値となる値を定義\n    \"predictor\":'gpu_predictor', #The type of predictor algorithm to use. Provides the same results but allows the use of GPU or CPU.\n    # Prediction using GPU. Used when tree_method is gpu_hist. When predictor is set to default value auto, the gpu_hist tree method is able to provide GPU based prediction without copying training data to GPU memory. If gpu_predictor is explicitly specified, then all data is copied into GPU, only recommended for performing prediction tasks.\n}\n\nclf = xgb.XGBClassifier()\nclf.set_params(**params)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T10:24:21.875754Z","iopub.execute_input":"2022-03-22T10:24:21.876145Z","iopub.status.idle":"2022-03-22T10:24:21.979671Z","shell.execute_reply.started":"2022-03-22T10:24:21.876109Z","shell.execute_reply":"2022-03-22T10:24:21.979014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgboost.train は n_estimators のパラメータを無視して num_boost_round で繰り返し回数が設定されるが、\n# xgboost.fit は n_estimators のパラメータを使う、のが違いとのことです。","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBRegressor and XGBClassifier is doable via underlying xgboost.fit function. \nparams_fit = {\n    \"eval_set\": [[X_val, y_val]],\n    \"eval_metric\": [\"merror\", \"mlogloss\"],#early_stopping_roundsが与えられているときに有効。検証データに対する評価基準で、デフォルトの基準はobjective によって決まります?\n    \"early_stopping_rounds\": 50,\n    \"verbose\": True,\n#     \"xgb_model\": None,\n#     \"sample_weight_eval_set\": None,\n#     \"base_margin_eval_set\": None,\n#     \"feature_weights\": None,\n#     \"callbacks\": None\n#     \"sample_weight\": None,\n#     \"base_margin\": None\n} #最後のfit()で使用。","metadata":{"execution":{"iopub.status.busy":"2022-03-22T10:25:40.24304Z","iopub.execute_input":"2022-03-22T10:25:40.243305Z","iopub.status.idle":"2022-03-22T10:25:40.247736Z","shell.execute_reply.started":"2022-03-22T10:25:40.243276Z","shell.execute_reply":"2022-03-22T10:25:40.247072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.fit(X_tr, y_tr, **params_fit)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T10:26:13.765142Z","iopub.execute_input":"2022-03-22T10:26:13.765392Z","iopub.status.idle":"2022-03-22T10:26:20.213978Z","shell.execute_reply.started":"2022-03-22T10:26:13.765363Z","shell.execute_reply":"2022-03-22T10:26:20.213153Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rslt_val_ = list(clf.evals_result().values())\nrslt_val = pd.DataFrame(dict(a_[0])).mean(axis=0)\nrslt_val","metadata":{"execution":{"iopub.status.busy":"2022-03-22T10:56:46.390746Z","iopub.execute_input":"2022-03-22T10:56:46.391029Z","iopub.status.idle":"2022-03-22T10:56:46.399339Z","shell.execute_reply.started":"2022-03-22T10:56:46.39099Z","shell.execute_reply":"2022-03-22T10:56:46.398701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(dict(a_[0])).mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T10:49:16.385342Z","iopub.execute_input":"2022-03-22T10:49:16.385665Z","iopub.status.idle":"2022-03-22T10:49:16.395281Z","shell.execute_reply.started":"2022-03-22T10:49:16.385619Z","shell.execute_reply":"2022-03-22T10:49:16.394531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.get_params(deep=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T10:59:25.591324Z","iopub.execute_input":"2022-03-22T10:59:25.591594Z","iopub.status.idle":"2022-03-22T10:59:25.599084Z","shell.execute_reply.started":"2022-03-22T10:59:25.591561Z","shell.execute_reply":"2022-03-22T10:59:25.598444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict(X, output_margin=False, ntree_limit=None,\n#         validate_features=True, base_margin=None, \n#         iteration_range=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport optuna\n\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\")\ny_train = train.target\nX_train = train.drop(['id','target'], axis=1)\nX_test_id = test[\"id\"]\nX_test = test.drop([\"id\"], axis=1)\nlbe = LabelEncoder() #ラベルエンコード  二次元での注意　https://gotutiyan.hatenablog.com/entry/2020/09/08/122621\ny_train_lbe = lbe.fit_transform(y_train) #.values.reshape(X.shape[0],1)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train_lbe, train_size=0.8, test_size=0.2, random_state=0)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-29T01:21:27.502344Z","iopub.execute_input":"2022-03-29T01:21:27.502875Z","iopub.status.idle":"2022-03-29T01:21:31.431303Z","shell.execute_reply.started":"2022-03-29T01:21:27.502782Z","shell.execute_reply":"2022-03-29T01:21:31.430569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    \n    param_opt = {\n#         'objective': ['multi:softprob'],\n#         'eval_metric': ['mlogloss'],\n#         'booster': ['gbtree'],\n#         \"verbosity\": [0],\n#         'random_state': [42],\n#         サーチしないパラメータは関数に直接書いた。\n        'max_depth': trial.suggest_int('max_depth', 2, 15),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n        'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n        'reg_alpha': trial.suggest_int('reg_alpha', 1, 50),\n        'reg_lambda': trial.suggest_int('reg_lambda', 5, 100),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 20),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n    }\n   \n\n    param_fit = {\n        \"eval_set\": [(X_val, y_val)],\n        \"eval_metric\": [\"mlogloss\", \"merror\"], # early_stopping_roundsが与えられているときに有効。検証データに対する評価基準で、デフォルトの基準はobjective によって決まります?\n        \"early_stopping_rounds\": 50,\n        \"verbose\": True,\n#       \"xgb_model\": None,\n#       \"sample_weight_eval_set\": None,\n#       \"base_margin_eval_set\": None,\n#       \"feature_weights\": None,\n#       \"callbacks\": None\n#       \"sample_weight\": None,\n#       \"base_margin\": None\n        }\n#         サーチしないパラメータは関数に直接書いた。\n    model = XGBClassifier(objective='multi:softprob',\n                          eval_metric='mlogloss',\n                          random_state=42, \n                          tree_method='gpu_hist', \n                          n_estimators=10000,\n                          gpu_id=0, \n                          predictor=\"gpu_predictor\",\n                          verbosity=0,\n                          **param_opt)  \n    model.fit(X_tr, y_tr, **param_fit)\n    preds = model.predict(X_val)\n    acc = accuracy_score(y_val, preds)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-03-27T03:30:58.042101Z","iopub.execute_input":"2022-03-27T03:30:58.042369Z","iopub.status.idle":"2022-03-27T03:30:58.053987Z","shell.execute_reply.started":"2022-03-27T03:30:58.042339Z","shell.execute_reply":"2022-03-27T03:30:58.052826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3)\n#study.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T03:32:02.924523Z","iopub.execute_input":"2022-03-27T03:32:02.924816Z","iopub.status.idle":"2022-03-27T03:33:20.751387Z","shell.execute_reply.started":"2022-03-27T03:32:02.924783Z","shell.execute_reply":"2022-03-27T03:33:20.750735Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)\n# print('Best trial:', study.best_trial.params)\n# print('Best trial:', study.best_trial.params)\n# print('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:27:44.472177Z","iopub.execute_input":"2022-03-24T08:27:44.472418Z","iopub.status.idle":"2022-03-24T08:27:44.480319Z","shell.execute_reply.started":"2022-03-24T08:27:44.472392Z","shell.execute_reply":"2022-03-24T08:27:44.479431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_trial","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:33:01.893411Z","iopub.execute_input":"2022-03-24T08:33:01.894012Z","iopub.status.idle":"2022-03-24T08:33:01.901525Z","shell.execute_reply.started":"2022-03-24T08:33:01.893977Z","shell.execute_reply":"2022-03-24T08:33:01.900834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# optunaでcrossvalidation","metadata":{}},{"cell_type":"code","source":"from sklearn import model_selection\ndef objective(trial):\n    \n    param_opt = {\n#         'objective': ['multi:softprob'],\n#         'eval_metric': ['mlogloss'],\n#         'booster': ['gbtree'],\n#         \"verbosity\": [0],\n#         'random_state': [42],\n#         サーチしないパラメータは関数に直接書いた。\n        'max_depth': trial.suggest_int('max_depth', 2, 15),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n        'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n        'reg_alpha': trial.suggest_int('reg_alpha', 1, 50),\n        'reg_lambda': trial.suggest_int('reg_lambda', 5, 100),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 20),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n    }\n   \n\n#      param_fit = {\n#         \"eval_set\": [(X_val, y_val)],\n#         \"eval_metric\": [\"mlogloss\", \"merror\"], # early_stopping_roundsが与えられているときに有効。検証データに対する評価基準で、デフォルトの基準はobjective によって決まります?\n#         \"early_stopping_rounds\": 50,\n#         \"verbose\": True,\n#       \"xgb_model\": None,\n#       \"sample_weight_eval_set\": None,\n#       \"base_margin_eval_set\": None,\n#       \"feature_weights\": None,\n#       \"callbacks\": None\n#       \"sample_weight\": None,\n#       \"base_margin\": None\n#         }\n#         サーチしないパラメータは関数に直接書いた。\n    model = XGBClassifier(objective='multi:softprob',\n                          eval_metric='mlogloss',\n                          random_state=42, \n                          tree_method='gpu_hist', \n                          n_estimators=10000,\n                          gpu_id=0, \n                          predictor=\"gpu_predictor\",\n                          verbosity=0,\n                          **param_opt)  \n#     model.fit(X_tr, y_tr, **param_fit)\n    score = model_selection.cross_val_score(model,\n                                            X_tr,\n                                            y_tr,\n                                            scoring='accuracy',\n                                            n_jobs=-1,\n                                            cv=10\n                                           )\n    acc = score.mean()\n#     preds = model.predict(X_val)\n#     acc = accuracy_score(y_val, preds)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-03-27T07:53:06.336836Z","iopub.execute_input":"2022-03-27T07:53:06.337101Z","iopub.status.idle":"2022-03-27T07:53:06.346597Z","shell.execute_reply.started":"2022-03-27T07:53:06.337074Z","shell.execute_reply":"2022-03-27T07:53:06.345914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(model, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T07:56:30.98368Z","iopub.execute_input":"2022-03-27T07:56:30.98441Z","iopub.status.idle":"2022-03-27T07:56:31.011096Z","shell.execute_reply.started":"2022-03-27T07:56:30.984355Z","shell.execute_reply":"2022-03-27T07:56:31.010029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# optunasearchcvを使った方法","metadata":{}},{"cell_type":"code","source":"type(y_val)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T00:58:09.345372Z","iopub.execute_input":"2022-03-29T00:58:09.345777Z","iopub.status.idle":"2022-03-29T00:58:09.35216Z","shell.execute_reply.started":"2022-03-29T00:58:09.345704Z","shell.execute_reply":"2022-03-29T00:58:09.351166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# openseacrcvを使った方法\n\n\n#Searchするハイパラ\nparam_search = {\n#         'objective': ['multi:softprob'],\n#         'eval_metric': ['mlogloss'],\n#         'booster': ['gbtree'],\n#         \"verbosity\": [0],\n#         'random_state': [42],\n#         サーチしないパラメータは関数に直接書いた。\n    \n    \n#           'max_depth': trial.suggest_int('max_depth', 2, 15),\n          \"max_depth\": optuna.distributions.IntLogUniformDistribution(2,50,1),\n#           'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n          'subsample': optuna.distributions.UniformDistribution(0.6, 1.0),\n#           'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n          'eta': optuna.distributions.UniformDistribution(0.01, 0.1),\n#           'reg_alpha': trial.suggest_int('reg_alpha', 1, 50),\n          'reg_alpha': optuna.distributions.IntUniformDistribution(1, 50, 1),\n#           'reg_lambda': trial.suggest_int('reg_lambda', 5, 100),\n          'reg_lambda': optuna.distributions.IntUniformDistribution(5, 100, 1),\n#           'min_child_weight': trial.suggest_int('min_child_weight', 2, 20),\n          'min_child_weight': optuna.distributions.IntUniformDistribution(2, 20, 1),\n#           \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n          \"colsample_bytree\": optuna.distributions.UniformDistribution(0.1, 1.0)\n    }\n\nfrom xgboost import XGBClassifier\nmodel = XGBClassifier(\n    objective='multi:softprob',\n    eval_metric='mlogloss',\n    use_label_encoder=False,\n    random_state=42, \n    tree_method='gpu_hist', \n    n_estimators=10000,\n    gpu_id=0, \n    predictor=\"gpu_predictor\",\n    verbosity=0\n) \n# OptunaSearchCV本体のパラメータ\nparam_opt_cv = {\n    'estimator': model,\n    'param_distributions': param_search,\n    'cv': 5,\n    'enable_pruning': False,\n    'error_score': np.nan,\n    'max_iter': 1000,\n    'n_trials': 3,\n    'random_state': None,\n    'refit': True,\n    'return_train_score': False,\n    'scoring': \"neg_log_loss\",\n    'study': None,\n    'subsample': 1.0,\n    'timeout': None,\n    'verbose': 10\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"# optunaSearchCVは実験的な位置づけ\noptuna_search = optuna.integration.OptunaSearchCV(**param_opt_cv)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T07:18:52.835409Z","iopub.execute_input":"2022-03-27T07:18:52.835929Z","iopub.status.idle":"2022-03-27T07:18:52.845267Z","shell.execute_reply.started":"2022-03-27T07:18:52.835885Z","shell.execute_reply":"2022-03-27T07:18:52.844473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna_search.fit(X_tr, y_tr)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T07:20:10.454309Z","iopub.execute_input":"2022-03-27T07:20:10.454629Z","iopub.status.idle":"2022-03-27T07:23:36.119857Z","shell.execute_reply.started":"2022-03-27T07:20:10.454597Z","shell.execute_reply":"2022-03-27T07:23:36.116617Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# cross_val（optunaでbest_paramを求めた後に）","metadata":{}},{"cell_type":"code","source":"# f1socreでエラー発生、acuでもエラー発生","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport optuna\n\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\")\ny_train = train.target\nX_train = train.drop(['id','target'], axis=1)\nX_test_id = test[\"id\"]\nX_test = test.drop([\"id\"], axis=1)\nlbe = LabelEncoder() #ラベルエンコード  二次元での注意　https://gotutiyan.hatenablog.com/entry/2020/09/08/122621\ny_train_lbe = lbe.fit_transform(y_train) #.values.reshape(X.shape[0],1)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train_lbe, train_size=0.8, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:59:22.345402Z","iopub.execute_input":"2022-03-30T09:59:22.345745Z","iopub.status.idle":"2022-03-30T09:59:26.143129Z","shell.execute_reply.started":"2022-03-30T09:59:22.345665Z","shell.execute_reply":"2022-03-30T09:59:26.142346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from xgboost import DMatrix\n# tr_d = DMatrix(X_tr, label=y_tr)\n# val_d = DMatrix(X_val, label=y_val)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T04:44:16.075604Z","iopub.execute_input":"2022-03-29T04:44:16.075917Z","iopub.status.idle":"2022-03-29T04:44:18.9199Z","shell.execute_reply.started":"2022-03-29T04:44:16.075885Z","shell.execute_reply":"2022-03-29T04:44:18.918782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\n# f1_macro = f1_score(y_true=y_val, y_pred=y_pred, average=\"macro\")\n\ndef f1_eval(y_pred, dtrain):\n    # 自作の評価関数はDmatrixで変換しないとeval_metricでエラーが出る？\n    y_true = dtrain.get_label()\n    score = f1_score(y_true, y_pred.argmax(axis=1), average=\"macro\")\n    return 'f1_score', score\n\n# def f1_eval2(y_pred, dtrain):\n#     # 自作の評価関数はDmatrixで変換しないとeval_metricでエラーが出る？\n#     y_true = dtrain.get_label()\n#     score = f1_score(y_true, y_pred.argmax(axis=1), average=\"macro\") * 2\n#     return 'f1_score_2', score\n\ndef objective(trial, data=X_tr,target= y_tr):\n    # trialオブジェクトはシームレスにインスタンス化され、optuna.study.Study.optimize（）メソッドの背後にある目的関数に渡されます\n    param = {\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n        'subsampele': trial.suggest_categorical('subsample', [0.6, 0.7, 0.8, 1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008, 0.01, 0.012, 0.014]),\n        'n_estimators': trial.suggest_categorical('n_estimators', [3000]),\n        'max_depth': trial.suggest_categorical('max_depth', [4,5,7,9,15]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)}\n    model = XGBClassifier(\n        objective='multi:softprob',\n        tree_method='gpu_hist',\n        predictor=\"gpu_predictor\",\n        use_label_encoder=False,\n        random_state=42,\n        verbosity=2, # 0 (silent), 1 (warning), 2 (info), 3 (debug).\n        **param)\n    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric=f1_eval, early_stopping_rounds=100,verbose=True)\n    y_pred = model.predict(X_val)\n    f1_macro = f1_score(y_true=y_val, y_pred=y_pred, average=\"macro\")\n#     acu = roc_auc_score(y_val, y_pred, multi_class=\"ovr\")\n    return f1_macro","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:59:40.72787Z","iopub.execute_input":"2022-03-30T09:59:40.728697Z","iopub.status.idle":"2022-03-30T09:59:40.741503Z","shell.execute_reply.started":"2022-03-30T09:59:40.728657Z","shell.execute_reply":"2022-03-30T09:59:40.738656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction = 'minimize', study_name = \"study_1\")\nstudy.optimize(objective,n_trials=3)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:59:58.454628Z","iopub.execute_input":"2022-03-30T09:59:58.454921Z","iopub.status.idle":"2022-03-30T10:00:29.066685Z","shell.execute_reply.started":"2022-03-30T09:59:58.454891Z","shell.execute_reply":"2022-03-30T10:00:29.065873Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:09:38.208005Z","iopub.execute_input":"2022-03-30T09:09:38.20834Z","iopub.status.idle":"2022-03-30T09:09:38.214595Z","shell.execute_reply.started":"2022-03-30T09:09:38.208308Z","shell.execute_reply":"2022-03-30T09:09:38.213395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#結構便利かも\noptuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:32:21.367268Z","iopub.execute_input":"2022-03-30T09:32:21.367892Z","iopub.status.idle":"2022-03-30T09:32:21.642555Z","shell.execute_reply.started":"2022-03-30T09:32:21.367851Z","shell.execute_reply":"2022-03-30T09:32:21.641812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_trial.params","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:09:50.41614Z","iopub.execute_input":"2022-03-30T09:09:50.416432Z","iopub.status.idle":"2022-03-30T09:09:50.421877Z","shell.execute_reply.started":"2022-03-30T09:09:50.416384Z","shell.execute_reply":"2022-03-30T09:09:50.42101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.trials_dataframe().head()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:10:45.498919Z","iopub.execute_input":"2022-03-30T09:10:45.499701Z","iopub.status.idle":"2022-03-30T09:10:45.527691Z","shell.execute_reply.started":"2022-03-30T09:10:45.499647Z","shell.execute_reply":"2022-03-30T09:10:45.526996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(study.best_trial)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:10:31.939848Z","iopub.execute_input":"2022-03-30T09:10:31.940506Z","iopub.status.idle":"2022-03-30T09:10:31.94757Z","shell.execute_reply.started":"2022-03-30T09:10:31.940438Z","shell.execute_reply":"2022-03-30T09:10:31.945748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_contour(study)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:18:35.947061Z","iopub.execute_input":"2022-03-30T09:18:35.947637Z","iopub.status.idle":"2022-03-30T09:18:37.994145Z","shell.execute_reply.started":"2022-03-30T09:18:35.947597Z","shell.execute_reply":"2022-03-30T09:18:37.993394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:14:07.921235Z","iopub.execute_input":"2022-03-30T09:14:07.921561Z","iopub.status.idle":"2022-03-30T09:14:08.051385Z","shell.execute_reply.started":"2022-03-30T09:14:07.921526Z","shell.execute_reply":"2022-03-30T09:14:08.050613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_parallel_coordinate(study)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:14:30.567768Z","iopub.execute_input":"2022-03-30T09:14:30.568062Z","iopub.status.idle":"2022-03-30T09:14:30.628555Z","shell.execute_reply.started":"2022-03-30T09:14:30.568033Z","shell.execute_reply":"2022-03-30T09:14:30.627877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"objective='multi:softprob',\n        tree_method='gpu_hist',\n        predictor=\"gpu_predictor\",\n        use_label_encoder=False,\n        random_state=42,\n        verbosity=2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nbest_params = study.best_params\nbest_params[\"objective\"]=\"multi:softprob\"\nbest_params[\"tree_method\"]=\"gpu_hist\"\nbest_params[\"predictor\"]=\"gpu_predictor\"\nbest_params[\"use_label_encoder\"]=False\nbest_params[\"random_state\"]=42\nbest_params[\"verbosity\"]=2\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:06:40.211511Z","iopub.execute_input":"2022-03-30T10:06:40.212082Z","iopub.status.idle":"2022-03-30T10:06:40.219428Z","shell.execute_reply.started":"2022-03-30T10:06:40.212043Z","shell.execute_reply":"2022-03-30T10:06:40.218654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_fin  =XGBClassifier(**best_params)\nmodel_fin.fit(X_train, y_train_lbe)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:06:55.811647Z","iopub.execute_input":"2022-03-30T10:06:55.812295Z","iopub.status.idle":"2022-03-30T10:08:03.841441Z","shell.execute_reply.started":"2022-03-30T10:06:55.812254Z","shell.execute_reply":"2022-03-30T10:08:03.840761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from optuna.study import StudySummary\nStudySummary(study_name=\"study_1\")","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:04:59.588785Z","iopub.execute_input":"2022-03-30T09:04:59.589338Z","iopub.status.idle":"2022-03-30T09:04:59.609865Z","shell.execute_reply.started":"2022-03-30T09:04:59.589297Z","shell.execute_reply":"2022-03-30T09:04:59.607897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = study.best_params\nstudy.StudySummary()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T01:30:19.689312Z","iopub.execute_input":"2022-03-29T01:30:19.689603Z","iopub.status.idle":"2022-03-29T01:30:19.860484Z","shell.execute_reply.started":"2022-03-29T01:30:19.689563Z","shell.execute_reply":"2022-03-29T01:30:19.858452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score\ndef cross_val(X, y, model, params, folds=10):\n    kf = KFold(n_splits=folds, shuffle=True, random_state=2022)\n    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"Fold: {fold}\")\n        X_tr, y_tr = X.values[train_idx], y.values[train_idx]\n        X_val, y_val = y.values[test_idx], y.values[test_idx]\n        alg = model(**params, random_state=2022)\n        alg.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n               early_stopping_rounds=400,\n               verbose=False)\n        y_pred = alg.predict(X_val)\n        f1_macro = f1_score(y_true=y_val, y_pred=y_pred, average=\"macro\")\n        print(f\"f1_macro:\"{f1_macro})\n        print(\"-\"*50)\n    return alg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_model = cross_val(X_test, y_test, model_best, param_best)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def cross_val(X, y, model, params, folds=10):\n\n#     kf = KFold(n_splits=folds, shuffle=True, random_state=2021)\n#     for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n#         print(f\"Fold: {fold}\")\n#         x_train, y_train = X.values[train_idx], y.values[train_idx]\n#         x_test, y_test = X.values[test_idx], y.values[test_idx]\n\n#         alg = model(**params,random_state = 2021)\n#         alg.fit(x_train, y_train,\n#                 eval_set=[(x_test, y_test)],\n#                 early_stopping_rounds=400,\n#                 verbose=False)\n#         pred = alg.predict(x_test)\n#         error = mean_squared_error(y_test, pred,squared = False)\n#         print(f\" mean_squared_error: {error}\")\n#         print(\"-\"*50)\n    \n#     return alg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import VotingRegressor\n# folds = KFold(n_splits = 10, random_state = 2021, shuffle = True)\n\n# predictions = np.zeros(len(test))\n\n# for fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n#     print(f\"Fold: {fold}\")\n#     X_train, X_val = X.values[trn_idx], X.values[val_idx]\n#     y_train, y_val = y.values[trn_idx], y.values[val_idx]\n\n#     model = VotingRegressor(\n#             estimators = [\n#                 ('lgbm', lgb),\n#                 ('xgb', xgb)\n#             ],\n#             weights = [0.15,0.25]\n#         )\n   \n#     model.fit(X_train, y_train)\n#     pred = model.predict(X_val)\n#     error = mean_squared_error(y_val, pred,squared = False)\n#     print(f\" mean_squared_error: {error}\")\n#     print(\"-\"*50)\n    \n#     predictions += model.predict(test) / folds.n_splits ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optunaとcross_valの組合せ\n# score = model_selection.cross_val_score(classifier_obj,\n#                                        X_tr,\n#                                        y_tr,\n#                                        n_jobs=-1,\n#                                        cv=3)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# estimator, param_distributions, cv=5,\n# enable_pruning=False, error_score=nan, max_iter=1000,\n# n_jobs=1, n_trials=10, random_state=None, refit=True,\n# return_train_score=False, scoring=None,\n# study=None, subsample=1.0, timeout=None, verbose=0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"objective='multi:softprob',\n        tree_method='gpu_hist',\n        predictor=\"gpu_predictor\",\n        use_label_encoder=False,\n        random_state=42,\n        verbosity=2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#searchするハイパラ\nimport optuna\n\nparam_search_opt = {\n    \"n_estimators\":optuna.distributions.IntUniformDistribution(10,1000,1),\n    \"max_depth\":optuna.distributions.IntLogUniformDistribution(2,50,1),\n    \"\"\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-27T04:48:38.371027Z","iopub.execute_input":"2022-03-27T04:48:38.371297Z","iopub.status.idle":"2022-03-27T04:48:38.376422Z","shell.execute_reply.started":"2022-03-27T04:48:38.371261Z","shell.execute_reply":"2022-03-27T04:48:38.375452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = XGBClassifier(objective='multi:softprob',\n#                     　eval_metric='mlogloss',\n#                     　random_state=42, \n#                     　tree_method='gpu_hist', \n#                     　n_estimators=10000,\n#                     　gpu_id=0, \n#                     　predictor=\"gpu_predictor\",\n#                     　verbosity=0,\n#                     　**param_opt) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna_search = optuna.integration.OptunaSearchCV(**param_opt_cv)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    #ステップ2\n    classifier_name = trial.suggest_categorical(\"classifier\", \n                                                [\"LogReg\",\n                                                 \"SVC\",\n                                                 \"RandomForest\"\n                                                ]\n                                               )\n    \n    #ステップ3\n    ##ロジ回（LogReg）\n    if classifier_name == 'LogReg':\n        logreg_c = trial.suggest_float(\"logreg_c\",\n                                       1e-10, 1e10, \n                                       log=True\n                                      )\n        classifier_obj = linear_model.LogisticRegression(C=logreg_c)\n    ##サポートベクターマシン（SVC）\n    elif classifier_name == \"SVC\":\n        svc_c = trial.suggest_float(\"svc_c\",\n                                    1e-10, 1e10,\n                                    log=True\n                                   )\n        classifier_obj = sklearn.svm.SVC(C=svc_c,\n                                         gamma=\"auto\"\n                                        )\n    ##ランダムフォレスト（RandomForest）\n    else:\n        rf_n_estimators = trial.suggest_int(\"rf_n_estimators\",\n                                            10, 1000\n                                           )\n        rf_max_depth = trial.suggest_int(\"rf_max_depth\", \n                                         2, 50,\n                                         log=True\n                                        )\n        classifier_obj = ensemble.RandomForestClassifier(\n            max_depth=rf_max_depth,\n            n_estimators=rf_n_estimators\n        )\n    #ステップ4\n    score = model_selection.cross_val_score(classifier_obj,\n                                            X,\n                                            y,\n                                            n_jobs=-1,\n                                            cv=10\n                                           )\n    accuracy = score.mean()\n    return accuracy\n# 目的関数の最適化を実行する（ステップ5）\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\n# 最適解の出力\nprint(f\"The best value is : \\n {study.best_value}\")\nprint(f\"The best parameters are : \\n {study.best_params}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CVあり","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\")\ny_train = train.target\nX_train = train.drop(['id','target'], axis=1)\nlbe = LabelEncoder() #ラベルエンコード  二次元での注意　https://gotutiyan.hatenablog.com/entry/2020/09/08/122621\ny_train_lbe = lbe.fit_transform(y_train) #.values.reshape(X.shape[0],1)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T03:25:11.355235Z","iopub.execute_input":"2022-03-24T03:25:11.355489Z","iopub.status.idle":"2022-03-24T03:25:12.646016Z","shell.execute_reply.started":"2022-03-24T03:25:11.35546Z","shell.execute_reply":"2022-03-24T03:25:12.645277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train_lbe, train_size=0.8, test_size=0.2, random_state=0)\n# valはRandomizedSearchCVのeval_setで使用。常に特定のデータセットであることに注意。","metadata":{"execution":{"iopub.status.busy":"2022-03-24T03:25:16.553448Z","iopub.execute_input":"2022-03-24T03:25:16.554122Z","iopub.status.idle":"2022-03-24T03:25:16.692482Z","shell.execute_reply.started":"2022-03-24T03:25:16.554083Z","shell.execute_reply":"2022-03-24T03:25:16.691389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5)\nfor tr_index, val_index in kf.split(X_tr, y_tr):\n    print(\"train_index:\", tr_index, \"validation_index:\", val_index)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T03:25:18.062796Z","iopub.execute_input":"2022-03-24T03:25:18.063271Z","iopub.status.idle":"2022-03-24T03:25:18.077113Z","shell.execute_reply.started":"2022-03-24T03:25:18.063236Z","shell.execute_reply":"2022-03-24T03:25:18.076423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RandomizedSearchCV(estimator, param_distributions, *, n_iter=10, scoring=None,\n#                  n_jobs=None, refit=True, cv=None, verbose=0,\n#                  pre_dispatch='2*n_jobs', random_state=None,\n#                  error_score=nan, return_train_score=False)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_rdm = {'objective': ['multi:softprob'],\n              'eval_metric': ['mlogloss'],\n              \"verbosity\": [0],\n              'n_estimators': [5],\n              'random_state': [42],\n              'booster': ['gbtree'],\n              \"tree_method\": [\"gpu_hist\"],\n              \"predictor\": ['gpu_predictor'],\n#               Invalid Input: 'u', valid values are: {'auto', 'cpu_predictor', 'gpu_predictor', 'oneapi_predictor'}\n              'learning_rate': [0.1],\n              'min_child_weight': [10],\n              'max_depth': [1,2,3,4,5,6,7,8,9,10],\n              'colsample_bytree': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n              'subsample': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n              }","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:41:26.9343Z","iopub.execute_input":"2022-03-24T04:41:26.934583Z","iopub.status.idle":"2022-03-24T04:41:26.94127Z","shell.execute_reply.started":"2022-03-24T04:41:26.934551Z","shell.execute_reply":"2022-03-24T04:41:26.940306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_fit = {\n    \"eval_set\": [(X_val, y_val)],\n    \"eval_metric\": [\"merror\", \"mlogloss\"], # early_stopping_roundsが与えられているときに有効。検証データに対する評価基準で、デフォルトの基準はobjective によって決まります?\n    \"early_stopping_rounds\": 50,\n    \"verbose\": True,\n#     \"xgb_model\": None,\n#     \"sample_weight_eval_set\": None,\n#     \"base_margin_eval_set\": None,\n#     \"feature_weights\": None,\n#     \"callbacks\": None\n#     \"sample_weight\": None,\n#     \"base_margin\": None\n} #最後のfit()で使用。","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:13:57.788886Z","iopub.execute_input":"2022-03-24T04:13:57.789701Z","iopub.status.idle":"2022-03-24T04:13:57.794875Z","shell.execute_reply.started":"2022-03-24T04:13:57.789656Z","shell.execute_reply":"2022-03-24T04:13:57.794199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # eval_metric\n# import xgboost as xgb\n# test__= xgb.XGBClassifier(eval_metric=\"mlogloss\", seed_per_iteration=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T03:20:07.930369Z","iopub.execute_input":"2022-03-24T03:20:07.931062Z","iopub.status.idle":"2022-03-24T03:20:07.935165Z","shell.execute_reply.started":"2022-03-24T03:20:07.931024Z","shell.execute_reply":"2022-03-24T03:20:07.934502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scoring = {'Log loss': 'neg_log_loss', 'AUC': 'roc_auc', 'F1': 'f1', 'Bal Acc': 'balanced_accuracy'}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nclf = xgb.XGBClassifier()\nclf_rds = RandomizedSearchCV(estimator=clf,\n                             param_distributions=params_rdm,\n                             n_iter=50,\n                             scoring=\"neg_log_loss\",\n                             n_jobs=-1,\n                             refit=\"neg_log_loss\",\n                             cv=kf,\n                             verbose=10,\n#                              pre_dispatch=,\n                             random_state=42,\n                             error_score=np.nan,\n                             return_train_score=False\n                             )","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:41:34.620565Z","iopub.execute_input":"2022-03-24T04:41:34.621113Z","iopub.status.idle":"2022-03-24T04:41:34.626855Z","shell.execute_reply.started":"2022-03-24T04:41:34.621078Z","shell.execute_reply":"2022-03-24T04:41:34.626058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_rds.fit(X_tr, y_tr, **params_fit)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:41:38.713887Z","iopub.execute_input":"2022-03-24T04:41:38.714139Z","iopub.status.idle":"2022-03-24T04:45:08.07069Z","shell.execute_reply.started":"2022-03-24T04:41:38.714111Z","shell.execute_reply":"2022-03-24T04:45:08.069824Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RandomizedSearchCVの付属機能\n\n# Atributes\n# cv_results_: dict of numpy (masked) ndarrays\n# best_estimator_: estimator\n# best_score_: float\n# best_params_: dict\n# best_index_: int\n# scorer_: function or a dict\n# n_splits_: int\n# refit_time_: float\n# multimetric_: bool\n# feature_names_in_: ndarray of shape\n# classes_\n# n_features_in_\n\n# Methods\n# decision_function(X)\n# fit(X[, y, groups])\n# get_params([deep])\n# inverse_transform(Xt)\n# predict(X)\n# predict_log_proba(X)\n# predict_proba(X)\n# score(X[, y])\n# score_samples(X)\n# set_params(**params)\n# transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoostClassifier\n# Property\n# coef_: numpy.ndarray\n# feature_importances_: numpy.ndarray\n# intercept_: numpy.ndarray\n    \n# Mmthod\n# apply(X, ntree_limit=0, iteration_range=None)\n# save_model(fname)\n# set_params(**params)\n# evals_result()\n# fit()\n# get_booster()\n# get_num_boosting_rounds()\n# get_params(deep=True)\n# get_xgb_params()\n# load_model(fname)\n# predict()\n# predict_proba()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 分類モデルの性能を測る指標。(このLog lossへの)入力は0~1の確率の値をとる。\n# この値を最小化したい。完璧なモデルではLog lossが0になる。\n# 予測値が正解ラベルから離れるほどLog lossは増加する。\n# Accuracyは予測した値と正解が一致していた数のカウント。正解/不正解しかないのでいつも良い指標とは限らない（惜しかった、などが測れない）\n# Log Lossは実際のラベルからどのくらい違っていたのかを考慮できる","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for score, params in ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_rds.n_features_in_","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:15:31.703032Z","iopub.execute_input":"2022-03-24T06:15:31.703278Z","iopub.status.idle":"2022-03-24T06:15:31.708784Z","shell.execute_reply.started":"2022-03-24T06:15:31.70325Z","shell.execute_reply":"2022-03-24T06:15:31.708105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_rds.feature_names_in_","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:19:20.128123Z","iopub.execute_input":"2022-03-24T06:19:20.128569Z","iopub.status.idle":"2022-03-24T06:19:20.147183Z","shell.execute_reply.started":"2022-03-24T06:19:20.128532Z","shell.execute_reply":"2022-03-24T06:19:20.14625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(clf_rds.best_score_)\nprint(clf_rds.best_params_)\nprint(clf_rds.best_index_)\nprint(clf_rds.scorer_)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:21:00.664314Z","iopub.execute_input":"2022-03-24T05:21:00.664565Z","iopub.status.idle":"2022-03-24T05:21:00.670233Z","shell.execute_reply.started":"2022-03-24T05:21:00.664537Z","shell.execute_reply":"2022-03-24T05:21:00.669512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop([\"id\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:56:41.324648Z","iopub.execute_input":"2022-03-24T05:56:41.325293Z","iopub.status.idle":"2022-03-24T05:56:41.348243Z","shell.execute_reply.started":"2022-03-24T05:56:41.325258Z","shell.execute_reply":"2022-03-24T05:56:41.347422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions\ny = clf_rds.predict(test)\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:57:21.491852Z","iopub.execute_input":"2022-03-24T05:57:21.492112Z","iopub.status.idle":"2022-03-24T05:57:21.681946Z","shell.execute_reply.started":"2022-03-24T05:57:21.492084Z","shell.execute_reply":"2022-03-24T05:57:21.681153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub[\"Id\"] = ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_rds.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:14:52.400889Z","iopub.execute_input":"2022-03-24T05:14:52.401407Z","iopub.status.idle":"2022-03-24T05:14:52.410921Z","shell.execute_reply.started":"2022-03-24T05:14:52.401373Z","shell.execute_reply":"2022-03-24T05:14:52.410245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_rds.cv_results_","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:13:11.536236Z","iopub.execute_input":"2022-03-24T05:13:11.536761Z","iopub.status.idle":"2022-03-24T05:13:11.581247Z","shell.execute_reply.started":"2022-03-24T05:13:11.536714Z","shell.execute_reply":"2022-03-24T05:13:11.580644Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.feature_importances_","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:10:11.067202Z","iopub.execute_input":"2022-03-24T05:10:11.067808Z","iopub.status.idle":"2022-03-24T05:10:11.088232Z","shell.execute_reply.started":"2022-03-24T05:10:11.067765Z","shell.execute_reply":"2022-03-24T05:10:11.087213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.best_score, clf.best_iteration.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# property \n# coef_\n# feature_importances_\n\n# method\n# evals_result()\n# get_booster()\n# get_num_boosting_rounds()\n# get_params(deep=True)\n# get_xgb_params()\n# load_model(fname)\n# predict()\n# predict_proba()\n# set_params(**params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGBオリジナルのAPIで使う。sklearnAPIでは使えない。\n# xgboost.train(params, \n#               dtrain,\n#               num_boost_round=10,\n#               evals=(), obj=None,\n#               feval=None,\n#               maximize=None,\n#               early_stopping_rounds=None,\n#               evals_result=None, verbose_eval=True,\n#               xgb_model=None, callbacks=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit(X, y, *, sample_weight=None, base_margin=None, \n#     eval_set=None, eval_metric=None, \n#     early_stopping_rounds=None, verbose=True, xgb_model=None, \n#     sample_weight_eval_set=None, \n#     base_margin_eval_set=None, feature_weights=None, callbacks=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGB\n# https://taka5hi.hatenablog.com/entry/2019/05/12/121633#Learning-Task-Parameters\nimport xgboost as xgb\n \nclf = xgb.XGBRegressor( \n        n_estimators=2000,\n        max_depth=6, \n        learning_rate=0.02,\n        subsample=0.8,\n        colsample_bytree=0.8, \n        missing=-1, \n        eval_metric='auc',\n        # USE CPU\n        # nthread=4,\n        # tree_method='hist' \n        # USE GPU\n        tree_method='gpu_hist' \n    )","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:56:13.354253Z","iopub.execute_input":"2022-03-12T07:56:13.354788Z","iopub.status.idle":"2022-03-12T07:56:13.359558Z","shell.execute_reply.started":"2022-03-12T07:56:13.354741Z","shell.execute_reply":"2022-03-12T07:56:13.358852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h = clf.fit(X_train.loc[idxT,cols], y_train[idxT], \n        eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n        verbose=50, early_stopping_rounds=100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:56:26.210262Z","iopub.execute_input":"2022-03-12T07:56:26.21073Z","iopub.status.idle":"2022-03-12T07:57:42.631724Z","shell.execute_reply.started":"2022-03-12T07:56:26.210692Z","shell.execute_reply":"2022-03-12T07:57:42.630522Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## optuna","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor as rfr\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.sparse import csr_matrix, csc_matrix, coo_matrix, lil_matrix\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\")\ny = train.target\nX = train.drop(['id','target'], axis=1)\nohe = OneHotEncoder(categories = 'auto') #ワンホット\nlbe = LabelEncoder() #ラベルエンコード\n# X_ohe = ohe.fit_transform(y.values.reshape(X.shape[0],1)).toarray() #Xにカテゴリ変数があれば使える。\n# X_sp = csr_matrix(X_ohe) #スパースな行列を圧縮させて容量を効率化させる\ny_lbe = lbe.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T09:45:16.60861Z","iopub.execute_input":"2022-03-19T09:45:16.609155Z","iopub.status.idle":"2022-03-19T09:45:19.434938Z","shell.execute_reply.started":"2022-03-19T09:45:16.609059Z","shell.execute_reply":"2022-03-19T09:45:19.434189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr, X_val, y_tr, y_val = train_test_split(X, y_lbe, train_size=0.8, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T09:46:18.8687Z","iopub.execute_input":"2022-03-19T09:46:18.869097Z","iopub.status.idle":"2022-03-19T09:46:19.012155Z","shell.execute_reply.started":"2022-03-19T09:46:18.869064Z","shell.execute_reply":"2022-03-19T09:46:19.011191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoostのLearningAPI\n データはDMatrixに変換させる必要がある","metadata":{}},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X_tr, label=y_tr) #labelはoheしたらエラーになる。lbeで代用\ndval = xgb.DMatrix(X_val, label=y_val)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:58:02.318757Z","iopub.execute_input":"2022-03-19T07:58:02.319003Z","iopub.status.idle":"2022-03-19T07:58:02.60471Z","shell.execute_reply.started":"2022-03-19T07:58:02.318976Z","shell.execute_reply":"2022-03-19T07:58:02.603757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optunaの前にxgb.cv()を試す\n# https://datatechlog.com/how_to_use_xgboost_and_optuna_for_parameter_optimization/#toc9\n# XGBのsklearn_api, python_api両方でGPUは使用できる。\nimport xgboost as xgb\nparams = {\n    \"n_estimators\": 10,\n    \"max_depht\": 5,\n    \"min_child_weight\": 1,\n    \"eta\": 0.1,\n    \"tree_method\": \"exact\",\n    \"objective\": \"reg:linear\",\n    \"eval_metric\": \"rmse\",\n    \"predictor\": \"gpu_predictor\" ,#\"cpu_predictor\"\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:05:49.637705Z","iopub.execute_input":"2022-03-19T07:05:49.637949Z","iopub.status.idle":"2022-03-19T07:05:49.642944Z","shell.execute_reply.started":"2022-03-19T07:05:49.637922Z","shell.execute_reply":"2022-03-19T07:05:49.641899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.sparse import csr_matrix, csc_matrix, coo_matrix, lil_matrix","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:40:01.499186Z","iopub.execute_input":"2022-03-19T07:40:01.500019Z","iopub.status.idle":"2022-03-19T07:40:01.503574Z","shell.execute_reply.started":"2022-03-19T07:40:01.499981Z","shell.execute_reply":"2022-03-19T07:40:01.502769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sp = csr_matrix(y)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:55:18.239117Z","iopub.execute_input":"2022-03-19T07:55:18.239671Z","iopub.status.idle":"2022-03-19T07:55:18.289533Z","shell.execute_reply.started":"2022-03-19T07:55:18.239639Z","shell.execute_reply":"2022-03-19T07:55:18.288309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_tr.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:30:14.644042Z","iopub.execute_input":"2022-03-19T07:30:14.644297Z","iopub.status.idle":"2022-03-19T07:30:14.64956Z","shell.execute_reply.started":"2022-03-19T07:30:14.644257Z","shell.execute_reply":"2022-03-19T07:30:14.648616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X_tr, label=y_tr)\ndval = xgb.DMatrix(X_val, label=y_val)\n\n# model = xgb.train(params=params,\n#                  dtrain=dtrain,\n#                  early_stopping_rounds=5,\n#                  evals=[(dtest, \"test\")])","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:29:33.138207Z","iopub.execute_input":"2022-03-19T07:29:33.138876Z","iopub.status.idle":"2022-03-19T07:29:33.410028Z","shell.execute_reply.started":"2022-03-19T07:29:33.138843Z","shell.execute_reply":"2022-03-19T07:29:33.409325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_tr.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:18:34.744477Z","iopub.execute_input":"2022-03-19T07:18:34.744759Z","iopub.status.idle":"2022-03-19T07:18:34.749655Z","shell.execute_reply.started":"2022-03-19T07:18:34.744732Z","shell.execute_reply":"2022-03-19T07:18:34.748994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pycuda","metadata":{"execution":{"iopub.status.busy":"2022-03-19T08:40:09.945997Z","iopub.execute_input":"2022-03-19T08:40:09.946227Z","iopub.status.idle":"2022-03-19T08:40:09.950459Z","shell.execute_reply.started":"2022-03-19T08:40:09.946199Z","shell.execute_reply":"2022-03-19T08:40:09.949672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:18:53.980007Z","iopub.execute_input":"2022-03-19T07:18:53.980259Z","iopub.status.idle":"2022-03-19T07:18:53.985557Z","shell.execute_reply.started":"2022-03-19T07:18:53.980232Z","shell.execute_reply":"2022-03-19T07:18:53.984757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\nmodel = xgb.train(params=params,\n                  dtrain=dtrain,\n                  early_stopping_rounds=5,\n                  evals=[(dtest, \"test\")])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\n#目的関数を定義\ndef objective(trial):\n    x = trial.suggest_uniform('x', -2, 2)\n    return 3*x**4 - 2*x**3 - 4*x**2 + 2\n#最適化を実行\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T05:15:25.426154Z","iopub.execute_input":"2022-03-14T05:15:25.426489Z","iopub.status.idle":"2022-03-14T05:15:26.370463Z","shell.execute_reply.started":"2022-03-14T05:15:25.426457Z","shell.execute_reply":"2022-03-14T05:15:26.369447Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2022-03-18T08:49:55.00202Z","iopub.execute_input":"2022-03-18T08:49:55.002471Z","iopub.status.idle":"2022-03-18T08:49:55.510208Z","shell.execute_reply.started":"2022-03-18T08:49:55.002432Z","shell.execute_reply":"2022-03-18T08:49:55.509507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-03-14T05:15:50.840592Z","iopub.execute_input":"2022-03-14T05:15:50.840899Z","iopub.status.idle":"2022-03-14T05:15:50.850448Z","shell.execute_reply.started":"2022-03-14T05:15:50.840867Z","shell.execute_reply":"2022-03-14T05:15:50.849479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport sqlite3\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"white\", color_codes=True)\ncon = sqlite3.connect(\"/kaggle/database.sqlite\")","metadata":{"execution":{"iopub.status.busy":"2022-03-14T05:34:23.562142Z","iopub.execute_input":"2022-03-14T05:34:23.562485Z","iopub.status.idle":"2022-03-14T05:34:23.570055Z","shell.execute_reply.started":"2022-03-14T05:34:23.562454Z","shell.execute_reply":"2022-03-14T05:34:23.569029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## sqlite","metadata":{}},{"cell_type":"code","source":"import sqlite3\nfrom contextlib import closing\nimport pandas as pd\ncreate_table = '''create table sample_table (氏名 text, 年齢 integer);'''\ndbname = \"/kaggle/sample_table.db\"\n\nwith closing(sqlite3.connect(dbname)) as conn:\n    c = conn.cursor()\n    c.execute(create_table)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-14T05:48:39.788539Z","iopub.execute_input":"2022-03-14T05:48:39.789215Z","iopub.status.idle":"2022-03-14T05:48:39.810244Z","shell.execute_reply.started":"2022-03-14T05:48:39.78918Z","shell.execute_reply":"2022-03-14T05:48:39.809329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with closing(sqlite3.connect(dbname)) as conn:\n    conn.execute(\"insert into sample_table values(?, ?)\", [\"高橋花子\", 20])\n    conn.commit()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T06:11:17.569172Z","iopub.execute_input":"2022-03-14T06:11:17.569731Z","iopub.status.idle":"2022-03-14T06:11:17.590946Z","shell.execute_reply.started":"2022-03-14T06:11:17.569683Z","shell.execute_reply":"2022-03-14T06:11:17.589983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with sqlite3.connect(dbname) as conn:\n    df = pd.read_sql(\"select * from sample_table\", con = conn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are 4 tables\npost = pd.read_sql_query(\"SELECT * FROM post\", con)\ncomment = pd.read_sql_query(\"SELECT * FROM comment\", con)\nlike = pd.read_sql_query(\"SELECT * FROM like\", con)\n\n# We don't want url. That just displays the image for the person.\n# Also need a column name change.\nrmember = pd.read_sql_query(\"SELECT distinct id as rid, name  rname FROM member\", con)\n\n# We'll update comment to add the response name (rname) to comment\ncomment=pd.merge(comment, rmember, left_on='rid', right_on='rid',how='left')","metadata":{"execution":{"iopub.status.busy":"2022-03-14T05:39:21.179024Z","iopub.execute_input":"2022-03-14T05:39:21.179367Z","iopub.status.idle":"2022-03-14T05:39:21.20802Z","shell.execute_reply.started":"2022-03-14T05:39:21.179334Z","shell.execute_reply":"2022-03-14T05:39:21.206482Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"post = pd.read_sql_query(\"SELECT * FROM post\", con)\ncomment = pd.read_sql_query(\"SELECT * FROM comment\", con)\nlike = pd.read_sql_query(\"SELECT * FROM like\", con)\nremember = pd.read_sql_query(\"SELECT distinct id as rid, name  rname FROM member\", con)\ncomment=pd.merge(comment, rmember, left_on='rid', right_on='rid',how='left')","metadata":{"execution":{"iopub.status.busy":"2022-03-14T05:38:20.322153Z","iopub.execute_input":"2022-03-14T05:38:20.32324Z","iopub.status.idle":"2022-03-14T05:38:20.367389Z","shell.execute_reply.started":"2022-03-14T05:38:20.32317Z","shell.execute_reply":"2022-03-14T05:38:20.365743Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 線形回帰","metadata":{}},{"cell_type":"code","source":"X_tr = train.iloc[:,1:76]\nX_tr.head(2)\ny_tr = train[\"target\"]\ny_tr","metadata":{"execution":{"iopub.status.busy":"2022-03-14T07:53:48.929184Z","iopub.execute_input":"2022-03-14T07:53:48.929472Z","iopub.status.idle":"2022-03-14T07:53:48.987259Z","shell.execute_reply.started":"2022-03-14T07:53:48.929439Z","shell.execute_reply":"2022-03-14T07:53:48.986219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_enc = le.fit_transform(y_tr)\ny_enc","metadata":{"execution":{"iopub.status.busy":"2022-03-14T07:58:55.712026Z","iopub.execute_input":"2022-03-14T07:58:55.712365Z","iopub.status.idle":"2022-03-14T07:58:55.782466Z","shell.execute_reply.started":"2022-03-14T07:58:55.712336Z","shell.execute_reply":"2022-03-14T07:58:55.781273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#定義の違いを明確に理解する\n# 評価関数（Evaluation function）とは、機械学習モデルの性能（performance、精度）をスコアリング（＝点数化）して測定するための関数\n# 損失関数（Loss function）とは、「正解値」と、モデルによる出力された「予測値」とのズレの大きさ（これを「Loss：損失」と呼ぶ）を計算するための関数\n# 誤差逆伝播法では、損失関数は誤差関数（Error function）とも呼ばれる。\n# 勾配降下法は一気に勾配を求めるが、誤差逆伝播法は連鎖律を用いて段階的に勾配を求める","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nimport pandas as pd\nimport numpy as np\n\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nparams1 = {}\nmodel = LinearRegression(**params1)\nparams2 = {\"estimator\": model,\n           \"X\": X_tr,\n           \"y\": y_enc,\n           \"n_jobs\": -1,\n           \"verbose\": 2,\n           \"scoring\": [\"neg_mean_squared_error\",\n                       \"r2\"],\n           \"cv\": kf}\n#corss_validateはscoringを複数設定できる！！  score = [\"accuracy\", ... ,\"recall\"]の様にして\ncross_val = cross_validate(**params2)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T09:14:19.837444Z","iopub.execute_input":"2022-03-14T09:14:19.83779Z","iopub.status.idle":"2022-03-14T09:14:24.912249Z","shell.execute_reply.started":"2022-03-14T09:14:19.83776Z","shell.execute_reply":"2022-03-14T09:14:24.909977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_val","metadata":{"execution":{"iopub.status.busy":"2022-03-14T09:19:11.464764Z","iopub.execute_input":"2022-03-14T09:19:11.465057Z","iopub.status.idle":"2022-03-14T09:19:11.474612Z","shell.execute_reply.started":"2022-03-14T09:19:11.465023Z","shell.execute_reply":"2022-03-14T09:19:11.473549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_val[\"test_r2\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T09:21:36.567557Z","iopub.execute_input":"2022-03-14T09:21:36.568035Z","iopub.status.idle":"2022-03-14T09:21:36.57595Z","shell.execute_reply.started":"2022-03-14T09:21:36.56797Z","shell.execute_reply":"2022-03-14T09:21:36.574699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearch\nparam_grid = {\"n_jobs\": -1,\n              \"\"}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://blog.amedama.jp/entry/sklearn-cv-custom-metric\n# https://qiita.com/tomov3/items/039d4271ed30490edf7b\n# https://data-bunseki.com/2019/07/30/%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E8%AA%BF%E6%95%B4%EF%BC%88%E5%9B%9E%E5%B8%B0%EF%BC%89/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}